{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cGyUZ9o-ToWB"
      },
      "source": [
        "Copyright 2024 Luiz Barboza, Dale Bowman, Natasha A. Sahr, Vasile Rus, Andrew M. Olney and made available under [CC BY-SA]\n",
        "(https://creativecommons.org/licenses/by-sa/4.0) for text and [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0) for code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Educational Material: Classifiers\n",
        "\n",
        "## The goal of this educational material\n",
        "\n",
        "Educators implementing practical activities in an introductory class on classifiers will engage students in understanding the fundamental concepts of classification. Logistic regression, a key focus of the course, will be explored as a regression technique primarily used for binary classification, predicting the probability of an observation belonging to a positive class. The logistic regression's mathematical foundation, transitioning from linear regression through the sigmoid function to handle categorical outcomes, will be explained to provide students with a comprehensive understanding.\n",
        "\n",
        "The module will elaborate on the versatility of logistic regression, applicable to various classification tasks such as identifying diabetes, spam emails, or other binary outcomes. Educators will stress that logistic regression's coefficients can be interpreted probabilistically, indicating the impact of predictor variables on the likelihood of positive classification. The distinction between binary, multinomial, and ordinal logistic regression will be conveyed, guiding students on appropriate usage.\n",
        "\n",
        "Logistic regression's performance evaluation will be covered, emphasizing metrics such as accuracy, precision, and recall. The trade-off between precision and recall will be explained, highlighting their significance in scenarios where balancing the cost of false positives and false negatives is crucial, as seen in medical diagnosis. The course will conclude with insights into adjusting classification thresholds and the impact on precision and recall, providing educators with practical knowledge for effective model performance assessment and application."
      ],
      "metadata": {
        "id": "DmIjeY6VBvfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The proposed practice\n",
        "\n",
        "In this educational experiment, educators are guiding students through the process of using Logistic Regression as a classifier to predict whether bank customers will default based on historical data. The dataset contains information about customers' salary, balance, and a binary default column (0 for no default, 1 for default). The educators provide access to training and testing datasets through URLs, emphasizing the importance of data exploration and loading using Blockly.\n",
        "\n",
        "Students learn to use the scikit-learn library in Python, specifically the LogisticRegression class, to create and train a model. The code demonstrates the fitting process using the `fit()` method with salary and balance as features and default as the label. The educators encourage students to understand that the result is an object representing the trained model.\n",
        "\n",
        "To evaluate the model's performance, students calculate accuracy using the `score()` method on the training dataset, and subsequently on the testing dataset. The demonstration shows how to use the accuracy_score function from scikit-learn to compare predicted values with the actual default values in the testing dataset. In the provided example, both training and testing accuracies yield a perfect score of 100%, indicating the model's proficiency in predicting customer defaults based on salary and balance information.\n",
        "\n",
        "This practical activity enables students to apply machine learning concepts, reinforcing the steps of model training, prediction, and evaluation. The educators highlight the importance of understanding accuracy metrics and visually inspecting the differences between predicted and expected values in a real-world business scenario. Overall, this experiment provides a hands-on approach to machine learning with a focus on logistic regression and model assessment."
      ],
      "metadata": {
        "id": "gzNv7kCQCDvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression (Classifier)\n",
        "\n",
        "We have a file with historical customers of a bank, with information about their **salary** and **balance**, besides the information on whether the customer **defaulted** or not. This last column is the one that we are trying to train the model to predict, which is a categorical one, 0 (zero) for the customers who never defaulted and 1 (one) for those who defaulted at some point with the bank. The idea with the Logistic Regression uses the training dataset so the model can \"learn\" how to predict the default classification, based on historical information on salary and balance. Here is the path for the published training and testing datasets: `https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/banking_tr.csv` and `https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/banking_ts.csv` (Almost the same, the training one ends with tr and the testing one with ts). Similar to what was done previously, we can load this data using Blockly. With the training dataset loaded, it is possible to create an instance/object of the class **LogisticRegression** (logreg) from the **sklearn.linear_model** (skl) library. With this logreg object, it is possible to train the model using the `fit()` method, having as parameters the salary and balance as features and the default label, as follows:\n",
        "\n",
        "![logreg](https://pbs.twimg.com/media/GEE_-NaXEAAaA1n?format=jpg&name=medium)\n",
        "\n",
        "That will generate the Python code as shown below:\n"
      ],
      "metadata": {
        "id": "hdZbDUN4WDRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "import sklearn.linear_model as skl\n",
        "import pandas as pd\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/banking_tr.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/banking_ts.csv')\n",
        "logreg = skl.LogisticRegression()\n",
        "print(logreg.fit(train.get(['salary', 'balance']),train.default))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lywi-i2hglZR",
        "outputId": "675cf0c6-56e3-4fd1-816b-41f773dcd003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code does not generate a particular output. As a result, it is only the object, linreg, as a trained model. If you want to know how good the training was, you should calculate the ***accuracy***, using the **score()** method. Notice that the features and label columns are precisely as they were performed in the training (fit) previous step.\n",
        "\n",
        "![score](https://pbs.twimg.com/media/GEFANe_XwAAFf4i?format=jpg&name=medium)\n",
        "\n",
        "Generating the code below:"
      ],
      "metadata": {
        "id": "wvNkM5fGiqBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(logreg.score(train.get(['salary', 'balance']),train.default))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6QLfyOIiVuN",
        "outputId": "e4d23e76-030d-49da-cfb7-0ddef9c1a0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On our example, it shows a perfect accuracy, 100%. Being so, it is possible to predict if a client will potentially get defaulted based on his salary and balance from the testing dataset:\n",
        "\n",
        "![predict](https://pbs.twimg.com/media/GEFA8yAWAAADKdu?format=jpg&name=medium)\n",
        "\n",
        "\n",
        "As we can see on the code below:"
      ],
      "metadata": {
        "id": "wIDQKvzsj1vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(logreg.predict(test.get(['salary', 'balance'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM8raQBRjwBp",
        "outputId": "17094211-24db-4533-be70-96d80cb8acca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected value for the default column can be seen from the testing dataset. This allows you to visually inspect the difference between the predicted and expected values.\n",
        "\n",
        "A better way to do that is to calculate the **accuracy** for the testing dataset this time. To do that, we will have to use the `accuracy_score` function from the **sklearn.linear_model** (skm) library, as shown on the blocks below:\n",
        "\n",
        "![score](https://pbs.twimg.com/media/GEFBzzxWcAA0yBD?format=jpg&name=large)\n",
        "\n",
        "Which generates the following code:"
      ],
      "metadata": {
        "id": "SrPrhdJNkU9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(skm.accuracy_score(logreg.predict(test.get(['salary', 'balance'])),test.default))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBQ9hS96kTs_",
        "outputId": "3fdf9a67-7069-47f4-fac6-b31f5008582a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again a perfect accuracy, 100%, for our playtoy business scenario here!"
      ],
      "metadata": {
        "id": "yqZEw-yHlPQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPENDIX\n",
        "\n",
        "This appendix material can be used as a content target for students of this practice. If you think it's applicable, you can use it, provided the proper reference to this original transcript."
      ],
      "metadata": {
        "id": "R7fIvWOjAu-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro to Classifiers\n",
        "\n",
        "The classification problem, simply stated, is to assign a new observation to a class, where the classes have been specified in advance.\n",
        "There are many different types of classifiers as we will see throughout this course.\n",
        "\n",
        "The difference between classification and clustering is simple.\n",
        "In clustering, we don't know the classes in advance, so we group observations together to try to discover the classes.\n",
        "In classification, we know the classes in advance, so we train a model that can predict the class for any new observation.\n",
        "\n",
        "Clustering is an _unsupervised_ method since the correct classes are unknown.\n",
        "Classification is a _supervised_ method since the classes are known.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "So far, we have looked at two broad kinds of supervised learning, classification and regression.\n",
        "Classification predicts a class label for an observation (i.e., a row of the dataframe) and regression predicts a numeric value for an observation.\n",
        "\n",
        "Logistic regression is a kind of regression that is primarily used for classification, particularly binary classification.\n",
        "It does this by predicting the **probability** (technically the log-odds) of the positive class assigned label `1`.\n",
        "If the probability is above a threshold, e.g .50, then this predicted numeric value is interpreted as a classification of `1`.\n",
        "Otherwise, the predicted numeric value is interpreted as a classification of `0`.\n",
        "So **logistic regression predicts a numeric probability that we convert into a classification.**\n",
        "\n",
        "Logistic regression is widely used in data science classification tasks, for example to:\n",
        "\n",
        "* categorize a person as having diabetes or not having diabetes\n",
        "* categorize an incoming email as spam or not spam\n",
        "\n",
        "Because logistic regression is also regression, it captures the relationship between an outcome/dependent variable and the predictor/independent variables in a similar way to linear regression.\n",
        "The major difference is that the coefficients in logistic regression can be interpreted probabilistically, so that we can say how much more likely a predictor variable makes a positive classification.\n",
        "\n",
        "The most common kind of logistic regression is binary logistic regression, but it is possible to have:\n",
        "\n",
        "* Binary/binomial logistic regression\n",
        "* Multiclass/Multinomial logistic regression\n",
        "* Ordinal logistic regression (there is an order among the categories)\n",
        "\n",
        "## When to use logistic regression\n",
        "\n",
        "Logistic regression works best when you need a classifier and want to be able to interpret the predictor variables easily, as you can with linear regression.\n",
        "Because logistic regression is fundamentally regression, it has the some assumptions of linearity and additivity, which may not be appropriate for some problems.\n",
        "Binary logistic regression is widely used and scales well, but multinomial variants typically begin to have performance problems when the number of classes is large.\n",
        "\n",
        "## Mathematical Foundations of Logistic Regression for Binary Classification\n",
        "\n",
        "We briefly review in this section the mathematical formulation of logistic regression for binary classification problems.\n",
        "That is, the predicted categories are just two (say, 1 or 0) and each object or instance belongs to one and only one category.\n",
        "\n",
        "Logistic regression expresses the relationship between the output variable, also called dependent variable, and the predictors, also called independent variables or features, in a similar way to linear regression with an additional twist.\n",
        "The additional twist is necessary in order to transform the typical continuous value of linear regression onto a categorical value (0 or 1).\n",
        "\n",
        "**From Linear Regression to Logistic Regression**\n",
        "\n",
        "Let us review first the basics of linear regression and then discuss how to transform the mathematical formulation of linear regression such that the outcome is categorical.\n",
        "\n",
        "In a typical linear regression equation, the output variable $Y$ is related to $n$ predictor variables $X_j$ ($j=1,n$) using the following linear relation, where the output $Y$ is a linear combination of the predictors $X_j$ with corresponding weights (or coefficients) $\\beta_{j}$:\n",
        "\n",
        "$$Y = {\\beta}_{0} + \\sum \\limits _{j=1} ^{n} X_{j}{\\beta}_{j}$$\n",
        "\n",
        "In linear regression, the output $Y$ has continuous values between $-\\inf$ and $+\\inf$. In order to map such output values to just 0 and 1, we need to apply the sigmoid or logistic function.\n",
        "\n",
        "$$\\sigma (t) = \\frac{1}{1 + e^{-t}}$$\n",
        "\n",
        "A graphical representation of the sigmoid or logistic function is shown below (from Wikipedia).\n",
        "The important part is that the output values are in the interval $(0,1)$ which is close to our goal of just predicted values 1 or 0.\n",
        "\n",
        "![log](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png?20140704193223)\n",
        "\n",
        "When applied to the $Y = {\\beta}_{0} + \\sum \\limits _{j=1} ^{n} X_{j}{\\beta}_{j}$ from linear regression we get the following formulation for logistic regression:\n",
        "$$\\frac{1}{1 + e^{{\\beta}_{0} + \\sum \\limits _{j=1} ^{n} X_{j}{\\beta}_{j}}}$$\n",
        "\n",
        "The net effect is that the the typical linear regression output values ranging from $-\\inf$ and $+\\inf$ are now bound to $(0,1)$, which is typical for probabilities. That is, the above formulation can be interpreted as estimating the probability of instance $X$ (described by all predictors $X_j$) belonging to class 1.\n",
        "\n",
        "$$ P( Y=1 | X ) = \\frac{1}{1 + e^{{\\beta}_{0} + \\sum \\limits _{j=1} ^{p} X_{j}{\\beta}_{j}}}$$\n",
        "\n",
        "The probability of class 0 is then:\n",
        "\n",
        "$$ P( Y=0 | X ) = 1 - P( Y=1 | X ) $$\n",
        "\n",
        "Values close to 0 are deemed to belong to class 0 and values close to 1 are deemed to belong to class 1, thus resulting in a categorical output which is what we intend in logistic regression.\n",
        "\n",
        "## Interpreting the Coefficients in Logistic Regression\n",
        "\n",
        "One of the best ways to interpret the coefficients in logistic regression is to transform it back into a linear regression whose coefficients are easier to interpret.\n",
        "From the earlier formulation, we know that:\n",
        "\n",
        "$$ Y =  P( Y=1 | X ) = \\frac{1}{1 + e^{{\\beta}_{0} + \\sum \\limits _{j=1} ^{p} X_{j}{\\beta}_{j}}}$$\n",
        "\n",
        "Applying a log function on both sides, we get:\n",
        "\n",
        "$$ log \\frac{P ( Y=1 | X )}{1- P( Y=1 | X )} = \\sum \\limits _{j=1} ^{p}  X_{j}{\\beta}_{j} $$\n",
        "\n",
        "On the left-hand of the above expression we have the log odds defined as the ratio of the probability of class 1 versus the probability of class 0. Indeed, this expression $\\frac{P ( Y=1 | X )}{1- P( Y=1 | X )}$ is the odds because $1- P( Y=1 | X )$ is the probability of class 0, i.e., $P( Y=0 | X )$.\n",
        "\n",
        "Therefore, we conclude that the log odds are a linear regression of the predictor variables weighted by the coefficients $\\beta_{j}$. Each such coefficient therefore indicates a change in the log odds when the corresponding predictor changes with a unit (in the case of numerical predictors).\n",
        "\n",
        "You may feel more comfortable with probabilities than odds, but you have probably seen odds expressed frequently in the context of sports.\n",
        "Here are some examples:\n",
        "\n",
        "- 1 to 1 means 50% probability of winning\n",
        "- 2 to 1 means 67% probability of winning\n",
        "- 3 to 1 means 75% probability of winning\n",
        "- 4 to 1 means 80% probability of winning\n",
        "\n",
        "Odds are just the probability of success divided by the probability of failure.\n",
        "For example 75% probability of winning means 25% probability of losing, and $.75/.25=3$, and we say the odds are 3 to 1.\n",
        "\n",
        "Because log odds are not intuitive (for most people), it is common to interpret the coefficients of logistic regression as odds.\n",
        "When a log odds coefficient has been converted to odds (using $e^\\beta$), a coefficient of 1.5 means the positive class is 1.5 times more likely given a unit increase in the variable.\n",
        "\n",
        "## Peformance Evaluation\n",
        "\n",
        "Performance evaluation for logistic regression is  the same as for other classification methods.\n",
        "The typical performance metrics for classifiers are accuracy, precision, and recall (also called sensitivity).\n",
        "We previously talked about these, but we did not focus much on precision, so let's clarify that.\n",
        "\n",
        "In some of our previous classification examples, there are only two classes that are equally likely (each is 50% of the data).\n",
        "When classes are equally likely, we say they are **balanced**.\n",
        "If our classifier is correct 60% of the time with two balanced classes, we know it is 10% better than chance.\n",
        "\n",
        "However, sometimes things are very unbalanced.\n",
        "Suppose we're trying to detect a rare disease that occurs once in 10,000 people.\n",
        "In this case, a classifier that always predicts \"no disease\" will be correct 99.99% of the time.\n",
        "This is because the **true negatives** in the data are so much greater than the **true positives**\n",
        "Because the metrics of accuracy and specificity use true negatives, they can be somewhat misleading when classes are imbalanced.\n",
        "\n",
        "In contrast, precision and recall don't use true negatives at all (see the figure below).\n",
        "This makes them behave more consistently in both balanced and imbalance data.\n",
        "For these reasons, precision, recall, and their combination F1 (also called f-measure) are very popular in machine learning and data science.\n",
        "\n",
        "\n",
        "![matrix](https://pbs.twimg.com/media/GFrGd1_XwAE4a9e?format=jpg&name=medium)\n",
        "\n",
        "<!-- NOTE: this became redundant with Tasha's KNN classification notebook. I modified to amplify precision, which she did not focus much on. -->\n",
        "<!-- happens, it is easy\n",
        "\n",
        "These are typical derived by compared the predicted output to the golden or actual output/categories in the expert labelled dataset.\n",
        "\n",
        "For a binary classification case, we denote the category 1 as the positive category and category 0 as the negative category. Using this new terminology, When comparing the predicted categories to the actual categories we may end up with the following cases:\n",
        "* True Positives (TP): instances predicted as belonging to the positive category and which in fact do belong to the positive category\n",
        "* True Negatives (TN): instances predicted as belonging to the negative category and which in fact do belong to the negative category\n",
        "* False Positives (FP): instances predicted as belonging to the positive category and which in fact do belong to the negative category\n",
        "* False Negatives (FN): instances predicted as belonging to the negative category and which in fact do belong to the positive category\n",
        "\n",
        "From these categories, we define the following metrics:\n",
        "\n",
        "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "\n",
        "$Precision = \\frac{TP}{TP + FP}$\n",
        "\n",
        "$Recall = \\frac{TP}{TP + FN}$\n",
        "\n",
        "Classfication methods that have a high accuracy are preferred in general although  -->\n",
        "In some cases, maximizing precision or recall may be preferred.\n",
        "For instance, a high recall is highly recommended when making medical diagnosis since it is preferrable to err on mis-diagnosing someone as having cancer as opposed to missing someone who indeed has cancer, i.e., the method should try not to miss anyone who may indeed have cancer.\n",
        "This idea is sometimes referred to as **cost-sensitive classification**, because there may be an asymmetric cost toward making one kind of mistake vs. another (i.e. FN vs. FP).\n",
        "\n",
        "In general, there is a trade-off between precision and recall.\n",
        "If precision is high then recall is low and vice versa.\n",
        "Total recall (100% recall) is achievable by always predicting the positive class, i.e., label all instances as positive, in which case precision will be very low.\n",
        "\n",
        "In the case of logistic regression, you can imagine that we changed the threshold from .50 to a higher value like .90.\n",
        "This would make many observations previously classified as 1 now classified as 0.\n",
        "What was left of 1 would be very likely to be 1, since we are 90% confident (high precision).\n",
        "However, we would have lost all of the 1s between 50-90% (low recall).\n",
        "\n",
        "<!-- TODO: we need to normalize coverage of performance metrics across notebooks, particularly for classification -->"
      ],
      "metadata": {
        "id": "H6z_4awrA_YV"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "xpython",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}