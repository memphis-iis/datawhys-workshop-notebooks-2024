{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0TZUMvi6ZDl4"
      },
      "source": [
        "Copyright 2024 Luiz Barboza, Dale Bowman, Andrew M. Olney and made available under [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0) for text and [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0) for code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Educational Material: Regressors\n",
        "\n",
        "## The goal of this educational material\n",
        "\n",
        "Educators engaging in practical activities related to simple linear regression can focus on understanding and applying the fundamental concepts of this statistical method. The primary goal is to determine if there is a linear relationship between an independent variable (X) and a dependent variable (Y) and how this relationship can be used for prediction. They can explore the linear model, specifically ordinary least squares (OLS) regression, as a modeling tool to estimate the unknown parameters, including the intercept (ùõΩ0) and slope (ùõΩ1), from the data.\n",
        "\n",
        "Moving beyond simple linear regression, educators can introduce multiple linear regression for scenarios with multiple predictor variables. They can explore the main effects model and discuss the importance of additivity, where the effects of predictor variables contribute independently to the total effect on the dependent variable. The coefficient of determination (ùëü2) can be introduced as a measure to assess how well the model fits the data, with a focus on its interpretation.\n",
        "\n",
        "In summary, educators can guide students through the exploration and application of simple and multiple linear regression, covering key concepts, model diagnostics, and practical considerations in predictive modeling."
      ],
      "metadata": {
        "id": "74lfGPdHep7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The proposed practice\n",
        "\n",
        "In the pratical section of this material, educators will engage in practical activities aimed at utilizing linear regression for predicting students' performance in the third exam (AP3) based on their scores in the first two exams (AP1 and AP2). The historical grades dataset, with training and testing subsets, is loaded using Blockly. A Linear Regression model is instantiated and trained with the training dataset. The code snippet demonstrates the model fitting process with the `fit()` method, utilizing AP1 and AP2 as features and AP3 as the label.\n",
        "\n",
        "To assess the model's performance, the coefficient of determination (ùëü2) is calculated using the `score()` method on the training dataset. In this example, a high ùëü2 value indicates a strong fit, suggesting that AP1 and AP2 effectively explain the variations in AP3 for the training set. The educators then apply the trained model to predict AP3 grades for new students in the testing dataset. The predicted values are obtained using the `predict()` method. The subsequent step involves comparing these predictions with the actual values in the testing dataset, allowing for visual inspection of the differences.\n",
        "\n",
        "Furthermore, educators evaluate the model's performance on the testing dataset by calculating the ùëü2 score using the `r2_score` function from the sklearn.metrics library. The obtained ùëü2 score of 0.991 indicates a strong predictive capability for the testing dataset, reinforcing the model's reliability in predicting AP3 grades for new students. This practical exercise provides educators with hands-on experience in applying linear regression for educational data analysis and prediction."
      ],
      "metadata": {
        "id": "jGvODwd-ASWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression (Regressor)\n",
        "\n",
        "We have a file with students' historical grades, each with 3 (three) different exams, AP1, AP2, and AP3. All of them are numerical values, varying from 0 (zero) to 10 (ten). The idea with the Linear Regression uses the training dataset so the model can \"learn\" how to predict the third exam, AP3, based on the first two, AP1 and AP2. Here is the path for the published training and testing datasets,\n",
        "`https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/grades_tr.csv` and `https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/grades_ts.csv` (Almost the same, the training one ends with tr and the testing one with ts). Similiar to what was done previously, we can load this data using Blockly. With the training dataset loaded, it is possible to create a instance/object of the class **LinearRegression** (linreg) from the **sklearn.linear_model** (skl) library. With this linreg object, it is possible to train the model using the `fit()` method, having as parameters the AP1 and AP2 as features, and the AP3 as the label, as follows:\n",
        "\n",
        "![linreg](https://pbs.twimg.com/media/GD_00j2WIAA65Xv?format=jpg&name=medium)\n",
        "\n",
        "That will generate the Python code as shown below:\n"
      ],
      "metadata": {
        "id": "-hhOloT9VjZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "import sklearn.linear_model as skl\n",
        "import pandas as pd\n",
        "ttrr = pd.read_csv('https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/grades_tr.csv')\n",
        "ttss = pd.read_csv('https://raw.githubusercontent.com/memphis-iis/datawhys-workshop-notebooks-2024/main/datasets/grades_ts.csv')\n",
        "linreg = skl.LinearRegression()\n",
        "print(linreg.fit(ttrr.get(['AP1', 'AP2']),ttrr.AP3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TChnsaNsZj5V",
        "outputId": "d72aff53-006f-4ee4-9dc6-dfc61fbd4ef1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code does not generate a particular output. As a result, it is only the object, linreg, as a trained model. If you want to know how good the training was, or more technically, how much the features (AP1 and AP2) explain the label (AP3). For that you should calculate the performance using the `score()` method. Notice that the features and label columns are exactly as they were performed in the training (fit) previous step.\n",
        "\n",
        "\n",
        "![score](https://pbs.twimg.com/media/GD_3LlRXQAAhUvK?format=jpg&name=medium)\n",
        "\n",
        "Generating the code below:"
      ],
      "metadata": {
        "id": "4MZCl84HZy4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(linreg.score(ttrr.get(['AP1', 'AP2']),ttrr.AP3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzecXd34awzo",
        "outputId": "db0801c7-2530-4ec0-80d2-f9abb5c22efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.991007002580118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On our example, it shows a sufficiently strong  $r^2$, close to 1, for our business case, predict students grades (didactically). Being so, it is possible to predict AP3 grades from new students based on the AP1 and AP2 from the testing dataset:\n",
        "\n",
        "![predict](https://pbs.twimg.com/media/GD_4lAVWQAAZJBj?format=jpg&name=medium)\n",
        "\n",
        "\n",
        "As we can see on the code below:"
      ],
      "metadata": {
        "id": "Awe4QU6qbHKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(linreg.predict(ttss.get(['AP1', 'AP2'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeKsIYPib6yk",
        "outputId": "4366d8b4-4c4e-492e-a0da-fbe04984c68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9.51361104  9.86953858  9.69157481  9.58120883  9.47084286  9.36047688\n",
            "  9.2501109   4.36228727  4.25192129  4.14155531  4.03118934  4.21604414\n",
            "  4.10567816  3.99531219  3.88494621  3.77458023  3.66421425 10.38549131\n",
            "  9.86925418  9.90484694  9.94043969  9.97603244 10.0116252  10.04721795]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the testing dataset, it is possible to see the expected value for the AP3. With that, you can visually inspect the difference between the predicted and expected values.\n",
        "\n",
        "| AP1 | AP2 | AP3 |\n",
        "| ----|-----|-----|\n",
        "|4.60\t|4.60 | 3.60|\n",
        "|4.50\t|4.50\t| 3.50|\n",
        "|4.00\t|4.00 |\t4.00|\n",
        "|5.00\t|5.00\t| 5.00|\n",
        "|4.50\t|4.50\t| 4.50|\n",
        "|5.50\t|5.50\t| 5.50|\n",
        "|5.00\t|9.00\t| 10.0|\n",
        "|5.10\t|9.50\t| 10.0|\n",
        "|5.20\t|9.50\t| 10.0|\n",
        "|5.30\t|9.50\t| 10.0|\n",
        "|5.40\t|9.50\t| 10.0|\n",
        "|9.00\t|9.50\t| 9.50|\n",
        "|10.00|7.00\t| 5.00|\n",
        "|9.00\t|6.00\t| 4.00|\n",
        "|9.50\t|6.50\t| 4.50|\n",
        "|9.40 |6.40\t| 4.40|\n",
        "\n",
        "A better way to do that, is again to calculate the $r^2$, for the testing dataset this time. To do that you will have to use the **r2_score** function from the **sklearn.linear_model** (skm) library, as shown on the blocks below:\n",
        "\n",
        "![score](https://pbs.twimg.com/media/GD_4oPgXwAADboH?format=jpg&name=large)\n",
        "\n",
        "Which generates the following code:"
      ],
      "metadata": {
        "id": "E_qELrS1cCtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "print(skm.r2_score(linreg.predict(ttss.get(['AP1', 'AP2'])),ttss.AP3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wxz624Bcq6z",
        "outputId": "c2e34806-50e7-4c7f-ed96-8ecd28e17b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9909253946778697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again an excellent $r^2$ score for our example business scenario here, 0.99."
      ],
      "metadata": {
        "id": "kHRtSZ6ke4I-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPENDIX\n",
        "\n",
        "This appendix material can be used as a content target for students of this practice. If you think it's applicable, you can use it, provided the proper reference to this original transcript."
      ],
      "metadata": {
        "id": "3oMYiN0e-_lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple linear regression\n",
        "\n",
        "Simple linear regression attempts to answer the question: *Is the\n",
        "variable $X$ **linearly** related to the variable $Y$? ~~If so, what is\n",
        "the relationship and can we use it to predict $Y$?*~~ Here the variable\n",
        "$X$ is called the *predictor* or *explanatory* or *independent variable*\n",
        "or *covariate*.\n",
        "The variable $Y$ is called the *response* or *dependent variable*.\n",
        "\n",
        "### When to use simple linear regression\n",
        "\n",
        "Simple linear regression is useful when you want to predict the relationship between an independent variable and a dependent variable when it is reasonable to assume the relationship is linear.  It is also used when you want to predict the response variable for a new value of the predictor variable.\n",
        "\n",
        "## The Linear Model\n",
        "\n",
        "Simple linear regression, also called *ordinary least squares* (OLS), is a good modeling tool for data where there is a *linear* relationship between an independent variable, $X$, and a dependent variable $Y$.\n",
        "We can check for a linear relationship by plotting the variables in a scatterplot and trying to determine whether the points fall on and around a line (the same thing when looking for correlation between the variables).\n",
        "Statistical modeling involves proposing a theoretical relationship between variables and using data to estimate the unknown components, called **parameters**, of the model.\n",
        "The theoretical model used in simple linear regression is\n",
        "\n",
        "$$Y=\\beta_0 + \\beta_1 X +\\epsilon,$$\n",
        "\n",
        "where $\\beta_0$ is the intercept of the line (where it intersects the y-axis) and $\\beta_1$ is the slope of the line (how quickly it rises or falls).\n",
        "The term $\\epsilon$ is an error term assumed to be from a normal distribution with mean zero and (unknown) variance, $\\sigma^2$.\n",
        "Note that $\\beta_0, \\beta_1$ and $\\epsilon$ are the unknown parameters in this model.\n",
        "The slope and intercept are the parameters in the model that must be estimated from the data.\n",
        "\n",
        "The term ordinary least squares regression gets its name from the way that the intercept and slope are estimated.\n",
        "Figure 1 shows a scatterplot of engine displacement versus highway miles per gallon.\n",
        "The best fit regression line is the line drawn in black.\n",
        "This line is found so that the sum of the squared distances from the points to the line is minimized, hence the term *least squares*.\n",
        "One such distance from a point to the line is shown in red in Figure 1.\n",
        "\n",
        " ![image.png](https://pbs.twimg.com/media/GDg2aCcWgAA4yqc?format=png&name=small)\n",
        "\n",
        "Once we find the best fit regression line, we can find the *predicted* response values.\n",
        "If the estimate of the intercept is $b_0$ and the estimate of the slope is $b_1$, the predicted response at each predictor variable, $X$ is found as\n",
        "\n",
        "$$\\hat{Y} = b_0 + b_1 X.$$\n",
        "\n",
        "It is not recommended to predict the response for $X$ values outside of the range that was used to find the best fit line.\n",
        "The relationship between $Y$ and $X$ is only valid for the observed range of $X$ values.\n",
        "So, for example, if the values of $X$ used to fit the line are between 0 and 100, it would not make sense to predict $Y$ for values less than 0 or greater than 100.\n",
        "\n",
        "Since the estimates, $b_0$ and $b_1$, are calculated from the data, they are statistics and can be used to test whether there is a significant relationship between $X$ and $Y$.\n",
        "This type of test is called a Wald test and tests whether the true coefficient $\\beta_1$ is equal to zero or not.\n",
        "If $\\beta_1=0$, the theoretical model becomes $Y=\\beta_0 + \\epsilon$ and there is no relationship between $X$ and $Y.$\n",
        "The results of the Wald test will give a *p-value*.\n",
        "The p-value is a probability that measures how likely you are to get the estimate that you got (or something more extreme) if the true parameter $\\beta_1$ is really zero.\n",
        "The p-value is always between 0 and 1.\n",
        "For large p-values, it is more likely that $\\beta_1$ is equal to zero.\n",
        "For small p-values, it is more likely that $\\beta_1$ is not zero.\n",
        "So small p-values (typically less than 0.05) indicate that there is a strong linear relationship between $X$ and $Y$.\n",
        "\n",
        "It is worth stressing that simple linear regression is very similar to a concept you already know, the equation for a line:\n",
        "\n",
        "$$y = mx + b$$\n",
        "\n",
        "There are two trivial differences and one important difference.\n",
        "The two trivial differences are that $m$ is the same as $\\beta_1$ ($\\beta_1$ is the slope) and $b$ is the same as $\\beta_0$ ($\\beta_0$ is the intercept).\n",
        "The important difference is that simple linear regression **discovers** both $\\beta$s from the data using OLS.\n",
        "\n",
        "Unlike KNN regression, which only uses the $K$ closest datapoints to predict $\\hat{Y}$, simple linear regression uses *all* the datapoints to estimate the intercept $\\beta_0$ and the slope $\\beta_1$.\n",
        "Therefore we can describe simple linear regression as simpler than KNN regression (because it has fewer parameters; remember KNN regression has as many parameters as datapoints) and also more opinionated than KNN regression since linear regression assumes the data follows a line.\n"
      ],
      "metadata": {
        "id": "2dhKp624-_8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple linear regression  \n",
        "\n",
        "In many data sets there may be several predictor variables that have an effect on a response variable.\n",
        " In fact, the *interaction* between variables may also be used to predict response.\n",
        " When we incorporate these additional predictor variables into the analysis, the model is called *multiple regression* .\n",
        " The multiple regression model builds on the simple linear regression model by adding additional predictors with corresponding parameters.\n",
        "\n",
        "## What you will learn\n",
        "\n",
        "In the sections that follow you will learn about multiple regression, an extension of simple linear regression, and how it can be used to model your data for prediction.  We will study the following:\n",
        "\n",
        "- The multiple regression model\n",
        "- Interaction effects\n",
        "- Feature selection\n",
        "- Categorical variables\n",
        "- Diagnostics\n",
        "\n",
        "## When to use multiple regression\n",
        "\n",
        "Multiple regression models are useful when you have a continuous response variable and you believe there are multiple predictors that have a linear relationship with the response variable.  The ultimate goals of fitting a multiple regression model are to 1) predict the response for a new set of features and 2) determine which predictors are most influential on the response.\n",
        "\n",
        "\n",
        "## Multiple Regression Model\n",
        "Let's suppose we are interested in determining what factors might influence a baby's birth weight.\n",
        " In our data set we have information on birth weight, our response, and predictors: mother's age, weight and height and gestation period.\n",
        " A *main effects model*  includes each of the possible predictors but no interactions.\n",
        " Suppose we name these features as in the chart below.\n",
        "\n",
        "| Variable | Type  | Description       |\n",
        "|:--------|:-------|:-------------------|\n",
        "| BW       | Ratio | baby birth weight |\n",
        "| MA       | Ratio | mother's age      |\n",
        "| MW       | Ratio | mother's weight   |\n",
        "| MH       | Ratio | mother's height   |\n",
        "| GP       | Ratio | gestation period  |\n",
        "\n",
        "Then the theoretical main effects multiple regression model is\n",
        "\n",
        "$$BW = \\beta_0 + \\beta_1 MA + \\beta_2 MW + \\beta_3 MH + \\beta_4 GP+ \\epsilon.$$\n",
        "\n",
        "Now we have five parameters to estimate from the data, $\\beta_0, \\beta_1, \\beta_2, \\beta_3$ and $\\beta_4$.\n",
        " The random error term, $\\epsilon$ has the same interpretation as in simple linear regression and is assumed to come from a normal distribution with mean equal to zero and variance equal to $\\sigma^2$.\n",
        " Note that multiple regression also includes the polynomial models discussed in the simple linear regression notebook.\n",
        "\n",
        "One of the most important things to notice about the equation above is that each variable makes a contribution **independently** of the other variables.\n",
        "This is sometimes called **additivity**: the effects of predictor variable are added together to get the total effect on `BW`.\n",
        "\n",
        "## Categorical Variables\n",
        "\n",
        "In the birth weight example, there is also information available about the mother's activity level during her pregnancy.\n",
        " Values for this categorical variable are: low, moderate, and high.\n",
        " How can we incorporate these into the model?\n",
        " Since they are not numeric, we have to create *dummy variables*  that are numeric to use.\n",
        " A dummy variable represents the presence or absence of a level of the categorical variable by a 1 and the absence by a zero.\n",
        "  Fortunately, most software packages that do multiple regression do this for us automatically.\n",
        "\n",
        "Often, one of the levels of the categorical variable is considered the \"baseline\" and the contributions to the response of the other levels are in relation to baseline.\n",
        "Let's look at the data again.\n",
        " In the table below, the mother's age is dropped and the mother's activity level (MAL) is included.\n",
        "\n",
        " | Coefficients | Estimate | Std. Error | p-value  |\n",
        "|--------------|----------|------------|----------|\n",
        "| Intercept    | 31.35    | 4.65       | 3.68e-07 |\n",
        "| MW           | 2.74     | 0.82       | 0.0026   |\n",
        "| MH           | -0.04    | 0.02       | 0.0420   |\n",
        "| GP           | 1.11     | 1.03       | 0.2917   |\n",
        "| MALmoderate  | -2.97    | 1.44       | 0.049     |\n",
        "| MALhigh      | -1.45    | 2.69       | 0.5946   |\n",
        "\n",
        "\n",
        "For the categorical variable `MAL`,  `MALlow` has been chosen as the baseline.\n",
        " The other two levels have parameter estimates that we can use to determine which are significantly different from the low level.\n",
        " This makes sense because all mothers will at least have low activity level, and the two additional dummy variables `MALhigh` and `MALmoderate` just get added on top of that.\n",
        "\n",
        " We can see that `MAL` moderate level is significantly different from the low level (p-value < 0.05).\n",
        " The parameter estimate for the moderate level of `MAL` is -2.97.\n",
        " This can be interpreted as: being in the moderately active group decreases birth weight by 2.97 units compared to babies in the low activity group.\n",
        " We also see that for babies with mothers in the high activity group, their birth weights are not different from birth weights in the low group, since the p-value is not low (0.5946 &gt; 0.05) and so this term does not have a significant effect on the response (birth weight).\n",
        "\n",
        "  This example highlights a phenomenon that often happens in multiple regression.\n",
        " When we drop the variable `MA` (mother's age) from the model and the categorical variable is included, both `MW` (mother's weight) and `MH` (mother's height) are both important predictors of birth weight (p-values 0.0026 and 0.0420 respectively).\n",
        " This is why it is important to perform some systematic model selection (forward or backward or all possible) to find an optimum set of features.\n",
        "\n",
        "\n",
        "## Diagnostics\n",
        "\n",
        "As in the simple linear regression case, we can use the residuals to check the fit of the model.\n",
        " Recall that the residuals are the observed response minus the predicted response.\n",
        "\n",
        "  - Plot the residuals against each independent variable to check whether higher order terms are needed  \n",
        "  - Plot the residuals versus the predicted values to check whether the variance is constant  \n",
        "  - Plot a qq-plot of the residuals to check for normality  \n",
        "  \n",
        "## Multicollinearity\n",
        "\n",
        "Multicollinearity occurs when two variables or features are linearly related, i.e. they have very strong correlation between them (close to -1 or 1).\n",
        " Practically this means that some of the independent variables are measuring the same thing and are not needed.\n",
        " In the extreme case (close to -1 or 1), the estimates of the parameters of the model cannot be obtained.\n",
        " This is because there is no unique solution for OLS when multicolinearity occurs.\n",
        " As a result, multicollinearity makes conclusions about which features should be used questionable.\n",
        "\n",
        " ## Coefficient of Determination\n",
        "\n",
        "The *coefficient of determination*, $r^2$, is another way to assess the model fit.\n",
        "It measures the proportion of variation in the data that is due to the regression.\n",
        "The remaining variation is assumed to be due to random noise from the error term, $\\epsilon$.\n",
        "$r^2$ is always between 0 and 1.\n",
        "The bigger it is, the better the model fits the data."
      ],
      "metadata": {
        "id": "OgxSDDTE_azL"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "xpython",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}